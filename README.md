# ğŸ“„ RAG PDF App â€” Fast Local Retrieval-Augmented Generation with Groq API

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline that lets you **upload PDFs**, extract context, and **query them using LLMs**. It uses **Qdrant** for vector storage, **SentenceTransformer** for embeddings, and **Groqâ€™s open-source GPT-OSS-20B model** for ultra-fast responses.

---

## ğŸš€ Features

* PDF text extraction and chunking
* Embedding generation using huggingFace `all-MiniLM-L6-v2`
* Vector search with Qdrant
* Query answering using `openai/gpt-oss-20b` via Groq API
* Modular architecture with FastAPI + Streamlit UI + Inngest event functions

---

## ğŸ§© Project Structure

```
RAG_PDF_App/
â”œâ”€â”€ main.py                # FastAPI backend with RAG pipeline
â”œâ”€â”€ streamlit_app.py       # Frontend interface for users
â”œâ”€â”€ vector_db.py           # Qdrant vector database logic
â”œâ”€â”€ data_loader.py         # PDF loader, chunking, and embedding
â”œâ”€â”€ custom_types.py        # Pydantic models for data flow
â”œâ”€â”€ .env                   # Environment variables
â””â”€â”€ requirements.txt       # Python dependencies
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/yourusername/RAG_PDF_App.git
cd RAG_PDF_App
```

### 2ï¸âƒ£ Create and activate a virtual environment

```bash
python -m venv .venv
.venv\Scripts\activate  # For Windows
```

### 3ï¸âƒ£ Install dependencies

```bash
uv pip install -r requirements.txt
```

### 4ï¸âƒ£ Add environment variables

Create a file named `.env` in your project root:

```env
GROQ_API_KEY=gsk_your_groq_api_key_here
QDRANT_URL=http://localhost:6333
```

---

## ğŸ§  Running the App

### ğŸ–¥ï¸ 1. Run the backend (FastAPI)

```bash
uv run uvicorn main:app
```

Expected output:

```
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
```

### ğŸ§± Run Qdrant Locally via Docker

If you donâ€™t have Qdrant running yet, use Docker:

```bash
docker run -p 6333:6333 -p 6334:6334 \
    -v qdrant_storage:/qdrant/storage:z \
    qdrant/qdrant
``` 
Expected output:

```
âœ… Qdrant will now be available at: http://localhost:6333
```

### ğŸ§© 2. Run the Inngest worker (for async jobs)

```bash
npx inngest-cli@latest dev -u http://127.0.0.1:8000/api/inngest --no-discovery
```

Expected output:

```
HTTP Request: POST http://127.0.0.1:8288/fn/register "HTTP/1.1 200 OK"
```

### ğŸŒ 3. Run the Streamlit UI

```bash
uv run streamlit run .\streamlit_app.py
```

Then open your browser:

```
Local URL: http://localhost:8501
Network URL: http://10.x.x.x:8501
```

---

## ğŸ§© How It Works

1. **PDF Upload:** The user uploads a document via Streamlit.
2. **Chunking:** The text is split into chunks using `data_loader.py`.
3. **Embedding:** Each chunk is encoded using the `intfloat/e5-base-v2` model.
4. **Storage:** Embeddings are stored in Qdrant.
5. **Query:** The user asks a question â†’ The system searches for the most relevant chunks.
6. **Generation:** Context + Question â†’ Sent to `openai/gpt-oss-20b` model (Groq API).
7. **Response:** The model returns an accurate answer grounded in the document.

---

## âš¡ Groq Integration Details

We use Groqâ€™s **OpenAI-compatible API** endpoint:

```python
API_URL = "https://api.groq.com/openai/v1/chat/completions"
model = "openai/gpt-oss-20b"
```

Groqâ€™s API key format: `gsk_...`

This gives lightning-fast inference (â‰ˆ 100 tokens/sec) with no external dependencies.

---

## ğŸ§ª Test the Model Directly

Run this in PowerShell to verify your Groq API key:

```powershell
curl -X POST "https://api.groq.com/openai/v1/chat/completions" `
     -H "Authorization: Bearer gsk_your_groq_api_key_here" `
     -H "Content-Type: application/json" `
     -d '{"model": "openai/gpt-oss-20b", "messages": [{"role": "user", "content": "Hello from Groq!"}]}'
```

Expected output:

```json
{"choices":[{"message":{"role":"assistant","content":"Hello! How can I help you?"}}]}
```

---

## ğŸ§° Tech Stack

| Component       | Technology                       |
| --------------- | -------------------------------- |
| **Frontend**    | Streamlit                        |
| **Backend**     | FastAPI                          |
| **Vector DB**   | Qdrant                           |
| **Embeddings**  | SentenceTransformer (all-MiniLM-L6) |
| **LLM**         | openai/gpt-oss-20b (Groq API)    |
| **Async Jobs**  | Inngest                          |
| **Environment** | Python 3.12                      |

---

## ğŸ§¾ Example Workflow

1. Upload a PDF
2. Ingest it â†’ vectorized in Qdrant
3. Ask: â€œWhat are the main findings of section 3?â€
4. The system finds matching chunks
5. Sends context + question to Groq model
6. Displays the final summarized answer

---

## ğŸ§‘â€ğŸ’» Author

Built by **Chethan N.** â€” Innovating AI-powered retrieval syst
